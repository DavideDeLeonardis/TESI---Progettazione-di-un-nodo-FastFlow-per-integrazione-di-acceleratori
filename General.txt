// TESI: Integrazione FPGA in FastFlow

Si propone di realizzare un semplice prototipo per l'integrazione di kernel programmati su FPGA Alveo mediante Vitis (sostanzialmente kernel scritti in C/C++) nell'ambiente di programmazione FastFlow. Verranno utilizzati kernel esistenti.
L'obiettivo della tesi sarà quello di interfacciare i kernel mediante codice OpenCL sfruttando tutte le possibilità offerte da tale ambiente. L'integrazione in FastFlow dovrà permettere di utilizzare sia il classico parallelismo shared memory/thread sulla CPU multicore che l'offloading di computazioni pesanti sull'acceleratore FPGA.


FastFlow:
   https://github.com/fastflow/fastflow

Parte di colloquio con l'acceleratore vedi VITIS XILINX:
	https://docs.amd.com/r/en-US/Vitis-Tutorials-Getting-Started/Vitis-Introduction-and-Getting-Started

Per usarla con GPU (OpenCL)
    https://eunomia.dev/en/others/cuda-tutorial/15-opencl-vector-addition/

L'account è deleonardis, la macchina è
	pianosa.di.unipi.it has address 131.114.3.250







DONE!!!!!! ----- DEVO ANCORA RIUTILIZZARE I BUFFER di memoria sulla GPU/FPGA (su CPU sono riutilizzati):
   Spostare l'Allocazione: Spostare le chiamate clCreateBuffer dal metodo execute() al metodo initialize() degli adapter. I cl_mem (bufferA, bufferB, bufferC) diventerebbero membri privati delle classi GpuAccelerator e FpgaAccelerator.
   Semplificare execute: Il metodo execute() non dovrebbe più allocare e deallocare, ma solo orchestrare i trasferimenti di dati (Write, Read) e l'esecuzione del kernel, usando i buffer pre-allocati.
   Spostare la Deallocazione: Le chiamate clReleaseMemObject andrebbero spostate nel distruttore degli adapter.

   L'obiettivo è eliminare le costose operazioni di allocazione (clCreateBuffer) e deallocazione (clReleaseMemObject) della memoria per ogni singolo task, riutilizzando invece gli stessi buffer pre-allocati.




Punti chiave da ricordare:
   Asincronia: la computazione sull’acceleratore non blocca i thread FastFlow grazie alla coppia Producer/Consumer.
   Overlapping: mentre il kernel n è in esecuzione, il producer può preparare i dati per n + 1 e il consumer può leggere i risultati di n – 1.
   Portabilità: il flusso è identico per GPU (Apple) e FPGA (Xilinx); cambiano solo le funzioni specifiche usate dentro DeviceAdapter.

   Per dimostrare il valore dell'FPGA, devi confrontarlo con una CPU usata al massimo delle sue potenzialità, non con un singolo core.

   Dimostrare la Potenza di FastFlow: L'obiettivo è mostrare che FastFlow è un framework flessibile, capace di orchestrare in modo efficiente sia il parallelismo interno alla CPU sia quello esterno delegato a un acceleratore.

   Capire i Trade-Off: Implementando entrambe le versioni, sarai costretto a misurare e analizzare quando conviene davvero fare l'offloading. 
      Potresti scoprire che per problemi di piccole dimensioni, l'overhead del trasferimento dati verso l'FPGA rende la soluzione CPU-parallela più veloce.





// BUILD AND EXEC:
   rm -rf build; cmake -B build && cmake --build build && ./build/tesi-exec 1000000 7 gpu

// Install correct FF Version on VM:
   rm -rf external/fastflow; git clone https://github.com/fastflow/fastflow.git external/fastflow

// SYNC Mac files with VM pianos
   rsync -av --exclude={'build/','.DS_Store','.git/','.vscode/','General/','General.txt','Metrics commands.txt'} /Users/davidedeleonardis/Desktop/Tesi/ deleonardis@131.114.3.250:~/Tesi/


// COMPILAZIONE E LINKING SU FPGA per ottenere xclbin (in dir Tesi)
   unset XCL_EMULATION_MODE
   v++ -c -k krnl_heavy_compute --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target hw -o kernels/fpga/krnl_heavy_compute.xo kernels/fpga/krnl_heavy_compute.cpp
   v++ -l --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target hw -o kernels/fpga/krnl_heavy_compute.xclbin kernels/fpga/krnl_heavy_compute.xo

   NON CREA IL FILE xrt.run_summary ---- Linking con profiling per vitis_analyzer (vitis_analyzer --classic xrt.run_summary)
      v++ -l --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target hw --profile.data all:all:all -o krnl_vadd.xclbin kernels/krnl_vadd.xo


// COMPILAZIONE E LINKING SU FPGA se voglio usare SOFTWARE EMULATION per ottenere xclbin (in dir Tesi) + EXEC --------> OK!
   v++ -c -k krnl_vadd --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target sw_emu -o kernels/krnl_vadd.sw_emu.xo kernels/krnl_vadd.cpp
   v++ -l --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target sw_emu -o krnl_vadd.sw_emu.xclbin kernels/krnl_vadd.sw_emu.xo

   export XCL_EMULATION_MODE=sw_emu
   Modifica nome .xclbin in FpgaAccelerator in kernels/krnl_vadd.sw_emu.xclbin
   rm -rf build; cmake -B build && cmake --build build && ./build/tesi-exec 1000000 7 fpga

// COMPILAZIONE E LINKING SU FPGA se voglio usare HARDWARE EMULATION per ottenere xclbin (in dir Tesi) ----> e poi voglio usare VITIS ANALYZER       (Usa N piccoli)    ----> NON FUNZIONA!
   v++ -c -k krnl_vadd --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target hw_emu -o kernels/krnl_vadd.hw_emu.xo kernels/krnl_vadd.cpp
   v++ -l --platform xilinx_u50_gen3x16_xdma_5_202210_1 --target hw_emu --vivado.prop "run.my_run.is_locked=false" -o krnl_vadd.hw_emu.xclbin kernels/krnl_vadd.hw_emu.xo
      ERRORS:
         "IP ... is locked": Un componente hardware pre-costruito della sua piattaforma (pfm_top...) è "bloccato".
         "customized with software release 2022.1": Questo componente è stato creato e testato con la versione 2022.1 degli strumenti Xilinx.
         "has a different revision in the IP Catalog": La versione degli strumenti Vitis/Vivado che sta usando attualmente (probabilmente una più recente come la 2023.x) ha una versione più nuova di quel componente.
   export XCL_EMULATION_MODE=hw_emu
   Modifica nome .xclbin in FpgaAccelerator in kernels/krnl_vadd.hw_emu.xclbin
   rm -rf build; cmake -B build && cmake --build build && ./build/tesi-exec 1000000 7 fpga
   vitis_analyzer xrt.run_summary












2. Il Calcolo su GPU non è Sempre Vantaggioso (computed_time)
- Break-Even Point: Per problemi piccoli (N=1M), il computed_time della GPU è peggiore di quello della CPU. 
- Il tempo risparmiato grazie al parallelismo della GPU è inferiore al tempo speso per trasferire i dati avanti e indietro sulla memoria del dispositivo. 
- Il sistema raggiunge il "break-even" e diventa vantaggioso solo per N sufficientemente grandi.

Scalabilità non Lineare: Lo speedup del calcolo non aumenta linearmente con N. Passando da 16M a 67M elementi, lo speedup diminuisce da 1.89x a 1.20x. Questo è un punto di analisi molto interessante per la tesi. Possibili cause:
   - Il task (somma vettoriale) è molto semplice e limita la banda di memoria (memory-bound). 
   - Oltre una certa dimensione, sia la CPU che la GPU sono limitate non dalla loro potenza di calcolo, ma dalla velocità con cui possono leggere e scrivere dalla memoria.
   - La cache della CPU sull'M2 Pro è molto performante e potrebbe mitigare il vantaggio della GPU su problemi così lineari.


3. L'Overhead di Inizializzazione Domina il Tempo Totale (elapsed_time)
L'Overhead" per la GPU è grande
Questo costo è quasi interamente dovuto all'inizializzazione una tantum di OpenCL in svc_init, in particolare alla compilazione del kernel (clBuildProgram).
Poiché stiamo eseguendo un singolo task, questo costo fisso iniziale non viene mai ammortizzato e domina completamente il tempo di esecuzione totale. 
   => CON 100 TASK: il costo di setup dell'acceleratore viene ammortizzato, rendendo la soluzione GPU vincente in uno scenario a throughput elevato => OK!!!!
   Questo dimostra numericamente che l'enorme costo di setup iniziale (la compilazione del kernel, etc.) è stato "spalmato" con successo sui 100 task. 
   L'overhead rimanente è ora praticamente identico a quello della versione CPU, rappresentando il costo "puro" del framework FastFlow.


2. La Gerarchia delle Performance per la Somma Vettoriale
Per questo specifico task (somma vettoriale), i tuoi dati mostrano una chiara gerarchia di performance in termini di throughput:
   GPU > CPU > FPGA
Questo è un risultato controintuitivo e molto valido. Dimostra che non sempre l'hardware più specializzato è il più veloce e che la natura del problema è fondamentale.

3. Analisi: "Il Martello Giusto per il Chiodo Giusto"
Perché l'FPGA, un acceleratore hardware, è risultata più lenta della CPU?
- Task Memory-Bound: La somma vettoriale è un'operazione estremamente semplice, limitata non dalla potenza di calcolo, ma dalla velocità con cui si possono leggere e scrivere dati dalla memoria (memory-bound).



// Perché Non è Stata Usata una Pipeline FastFlow Interna?
Sebbene in teoria sembri più elegante annidare una ff_Pipe all'interno del ff_node_acc_t, questa soluzione è inferiore e più complessa a causa della gestione dello stato condiviso.
L'operazione di offloading richiede che lo stato di un singolo task (IAccelerator e le sue risorse come l'indice del buffer usato e gli eventi OpenCL per la sincronizzazione) sia mantenuto e passato coerentemente tra i nodi di Upload, Launch e Download.
Se avessimo usato tre nodi FastFlow separati (UploaderNode, LauncherNode, DownloaderNode), avremmo dovuto implementare complessi meccanismi di sincronizzazione esterni (ad esempio, mappe globali protette da mutex) per condividere l'oggetto accelerator e per associare lo stato corretto a ogni task che fluisce attraverso nodi indipendenti. 
Questo avrebbe reso il codice più difficile da leggere, più prono a errori (race condition) e avrebbe reintrodotto la complessità che il framework dovrebbe nascondere.
L'architettura attuale, con una pipeline manuale incapsulata in ff_node_acc_t, è superiore perché:
   Incapsula lo Stato: Il ff_node_acc_t "possiede" l'acceleratore e tutte le sue risorse (come il pool di buffer). Tutta la logica e lo stato sono confinati in un unico componente.
   Semplifica la Logica: La comunicazione tra i thread interni è banale, poiché condividono lo stato dello stesso oggetto.


// ff::ParallelFor
"CPU multicore": Lo scopo di ff::ParallelFor è proprio quello di distribuire il lavoro di un ciclo for su tutti i core disponibili della sua CPU. 
   Se ha 8 core, FastFlow creerà una squadra di 8 thread (o un numero ottimale) per eseguire il calcolo.

4. Lezione sulla Legge di Amdahl e l'Ammortamento
Questi risultati sono una perfetta dimostrazione pratica della Legge di Amdahl. La parte "seriale" del nostro programma (l'inizializzazione di OpenCL) è così lenta che, anche se acceleriamo la parte "parallela" (il calcolo), il guadagno totale è negativo per un singolo task.
Questo dimostra in modo conclusivo che l'offloading su acceleratori ha senso solo in scenari ad alto throughput, dove il costo di setup viene "spalmato" su un gran numero di operazioni.






Start session:
   screen -S vitis

Resume session:
   screen -r vitis   

See sessions:
   screen -ls

Delete session:
   screen -X -S [session # you want to kill] kill
   
Per ritornare in terminale da sessione:
   ctrl-a
   d



// L'Analisi del Problema - N troppo grande (PROBLEMA ANCHE IN FPGA)
The problem is the massive size of N you're trying to process in a single go. Let's do the math:
N (Dimensione del Vettore): 1,000,000,000 (1 miliardo di elementi)
Tipo di Dato: int (4 byte per elemento)
Numero di Vettori: 3 (input a, input b, output c)
The total memory you're asking the GPU to allocate is:
3 * 1,000,000,000 * 4 byte = 12,000,000,000 byte = 12 Gigabyte (GB).
Your Mac M2 Pro has a unified memory architecture (e.g., 16 GB or 32 GB total), but that memory is shared between the CPU, the GPU, the operating system, and all other running applications. 
Attempting to allocate a single chunk of 12 GB for your application is exceeding the amount of memory available to the GPU at that moment, causing the allocation to fail.

=> con n troppo grande la cpu dell'fpga ha spazio a disposizione per calcolare, MA fpga NO
   => FPGA ha 8GB di memoria a disposizione quindi dovrebbe essere in grado di calcolare con n=67108864 (805MB da utilizzare) ma l'OS non me lo permette
         xbutil examine --device 0000:81:00.1 --report memory







L'architettura a 3 stadi con questo nodo asincrono è instabile in FastFlow => uso 2 stadi (Emitter, accNode)





Ottimizzazione di CPU e GPU: Le CPU e le GPU moderne sono macchine potentissime e altamente ottimizzate per questo tipo di accesso lineare alla memoria, con cache sofisticate e bus di memoria molto ampi.
Forza dell'FPGA: La vera forza di un'FPGA non è eseguire task semplici, ma implementare pipeline di calcolo complesse e customizzate che non esistono su una CPU/GPU. Ad esempio, algoritmi di compressione video, elaborazione di segnali radio, crittografia, dove i dati fluiscono attraverso una catena di montaggio hardware su misura. Per un task così semplice, l'overhead di comunicazione con la scheda e la sua architettura di memoria generica non riescono a competere.
Usare un'FPGA per una somma vettoriale è come usare un bisturi chirurgico specializzato per aprire una scatola di cartone: funziona, ma un semplice taglierino (la CPU/GPU) è più efficiente per quel compito specifico.

4. Overhead di Inizializzazione
I tuoi log mostrano che anche l'FPGA ha un costo di inizializzazione significativo (elapsed - computed nel primo task), dovuto al tempo necessario per caricare il file .xclbin sulla scheda e configurare il circuito. Hai dimostrato che, come per la GPU, questo costo viene ammortizzato solo in scenari a throughput elevato.









Se volessi sostituire la somma vettoriale con un altro kernel, per esempio una moltiplicazione tra matrici, non dovresti toccare l'infrastruttura principale del progetto (ff_node_acc_t, la logica della pipeline, etc.). Le modifiche sarebbero mirate e contenute solo nelle parti specifiche del task.

File da Modificare
   1. I Nuovi File dei Kernel (L'Algoritmo)
   Prima di tutto, dovresti scrivere il nuovo algoritmo. Non modificheresti i file esistenti, ma ne creeresti di nuovi.
   Per la GPU: Creeresti un nuovo file, ad esempio matrix_mul.cl, contenente il kernel OpenCL C per la moltiplicazione di matrici.
   Per l'FPGA: Creeresti un nuovo file C++, ad esempio kernel/krnl_matmul.cpp, con la logica per la sintesi HLS della moltiplicazione di matrici.

   2. Il File dei Tipi Dati (include/types.hpp)
   La struttura Task attuale è specifica per la somma vettoriale (tre vettori, una dimensione). Una moltiplicazione di matrici ha bisogno di dati diversi.
   Azione: Aggiungeresti una nuova struct per descrivere il nuovo lavoro, per esempio:
   // Esempio per una moltiplicazione di matrici C = A x B
   struct MatrixMulTask {
      float *matrix_a, *matrix_b, *matrix_c;
      int width_a, height_a, width_b;
   };
   Anche la struct Result potrebbe dover cambiare se il risultato fosse più complesso.

   3. Le Classi "Adapter" (Cpu/Gpu/FpgaAccelerator.cpp)
   Qui è dove "collegheresti" il nuovo task e il nuovo kernel. Ogni adapter andrebbe modificato.
   CpuAccelerator.cpp:
   Nel metodo execute, la chiamata a std::transform verrebbe sostituita con due cicli for annidati per eseguire la moltiplicazione di matrici sulla CPU.
   GpuAccelerator.cpp e FpgaAccelerator.cpp:
   Nel metodo initialize, cambieresti il nome del file del kernel da caricare (es. matrix_mul.cl o il nuovo .xclbin) e il nome del kernel da estrarre (es. "matrix_mul").
   Nel metodo execute, faresti le modifiche più sostanziose:
   Faresti il cast del puntatore generic_task al nuovo tipo MatrixMulTask*.
   Le chiamate clCreateBuffer creerebbero buffer con le dimensioni corrette per le matrici.
   Le chiamate clSetKernelArg passerebbero i puntatori alle matrici e le loro dimensioni, in accordo con la firma del nuovo kernel.

   4. La Classe Emitter (in main.cpp)
   L'Emitter è responsabile di creare i dati di test.
   Azione: Modificheresti il costruttore dell'Emitter per inizializzare due matrici di input (a e b) con valori sensati. Anche la chiamata per creare il task diventerebbe return new MatrixMulTask{...};.


File che NON Andrebbero Modificati (La Prova della Buona Architettura)
   IAccelerator.hpp: L'interfaccia, il "contratto", non cambia. Definisce ancora le azioni generiche initialize ed execute.
   ff_node_acc_t.hpp e ff_node_acc_t.cpp: Il tuo "motore" asincrono a thread e code rimarrebbe identico. Non gli interessa quale calcolo venga eseguito; il suo compito è solo orchestrare il flusso di void* tra le code e delegare il lavoro all'acceleratore.
   La funzione main() in main.cpp: La logica di creazione della pipeline, la misurazione dei tempi e il meccanismo di verifica con promise/future rimarrebbero esattamente gli stessi. L'unica riga da cambiare sarebbe quella che crea l'adapter specifico (es. accelerator = std::make_unique<MatrixMulGpuAccelerator>();).





// In node Time = Compute Time + Overhead

1. Tempo Elapsed
Questo è il tempo completo che la tua pipeline impiega per processare tutti i task, dall'inizio alla fine. È la metrica che misura le performance dell'intero sistema software.
Cosa include:
   Il tempo di gestione della pipeline FastFlow.
   Il tempo che i Task e i Result passano nelle code (inQ_, outQ_).
   Il tempo di creazione e distruzione degli oggetti Task e Result.
   Il tempo di trasferimento dei dati dalla CPU all'acceleratore (per GPU e FPGA).
   Il tempo di calcolo vero e proprio (computed_time).
   Il tempo di trasferimento dei risultati dall'acceleratore alla CPU.

2. Tempo Computed (Tempo di Calcolo Effettivo della somma vettoriale)
Questo è il tempo speso esclusivamente dall'acceleratore per eseguire il calcolo. Misura la performance pura del "motore" di calcolo, isolandolo da tutto il resto dell'infrastruttura software.
Cosa include (per GPU e FPGA):
   Il tempo per copiare i dati dalla RAM del computer alla memoria della scheda (clEnqueueWriteBuffer).
   Il tempo per eseguire il kernel sulla scheda (clEnqueueNDRangeKernel).
   Il tempo per ricopiare i risultati dalla scheda alla RAM (clEnqueueReadBuffer).
   Il tempo per sincronizzare e assicurarsi che tutte le operazioni siano finite (clFinish).


Quando uso la GPU o l'FPGA, l'overhead è generato quasi interamente da due fattori:
Trasferimento Dati (la causa maggiore): È il tempo necessario per spostare i vettori di input dalla RAM del computer alla memoria dell'acceleratore (GPU/FPGA) e per poi copiare il risultato indietro. 
Questo viaggio, che avviene sul bus PCIe, è spesso il principale collo di bottiglia.

Gestione e Sincronizzazione: Include il tempo che la CPU impiega per inviare i comandi, il tempo che il driver del dispositivo impiega per 
schedulare il kernel e il tempo speso in attesa che l'operazione sia completata.

   Overhead = Tempo "Elapsed" - Tempo "Computed"
   Prendiamo uno dei tuoi test sulla GPU (Mac M2 Pro, N=1,000,000):
   Avg elapsed: 1.83 ms
   Avg computed: 0.94 ms
   Calcolo dell'overhead:
   1.83 ms - 0.94 ms = 0.89 ms
   Questo significa che per ogni task, il tuo sistema impiega 0.89 ms solo per spostare i dati e gestire l'operazione. 
   Il calcolo vero e proprio, invece, dura 0.94 ms. In questo caso, il costo di gestione è quasi pari al tempo di lavoro utile.


Avg In node Time (Latency) - Latenza Media di Servizio
   Misura il tempo totale che un singolo task trascorre all'interno del nodo acceleratore, dal suo ingresso (svc) fino al completamento del suo risultato (get_results_from_device). Rappresenta la latenza end-to-end percepita da una singola operazione.

Avg service time - Periodo Medio della Pipeline
   Misura il tempo medio che intercorre tra il completamento di un task e quello del task successivo quando la pipeline è a pieno regime. Questa metrica rivela la velocità dello stadio più lento del sistema e l'inverso della sua massima portata teorica.
   misura la portata massima del sistema a regime."


Throughput - Portata Complessiva
Indica il numero totale di task che l'intero sistema è in grado di processare al secondo, considerando il tempo totale di esecuzione. Misura l'efficienza e la capacità di elaborazione complessiva del sistema dall'inizio alla fine.
   [Immagine di una superstrada trafficata]

Avg Pure Compute Time - Tempo Medio di Calcolo Puro
   Isola il tempo effettivo che l'hardware dell'acceleratore (GPU o FPGA) impiega per eseguire il calcolo del kernel. Questa metrica esclude tutti i costi accessori come il trasferimento dati e le attese.

Avg Overhead Time - Overhead Medio
   Rappresenta il "costo di gestione" medio per ogni task, calcolato come la differenza tra la latenza totale e il tempo di calcolo puro. È dominato dal tempo speso per i trasferimenti di dati (I/O) e misura l'inefficienza che la pipeline cerca di nascondere.





LIMIT for N in VM: 7449999





La CPU fa il Manager: Quando l'uploaderLoop sulla CPU chiama send_data_to_device, non sta trasferendo i dati lui stesso. 
Sta dando un ordine al gestore della memoria del device (DMA) di iniziare il trasferimento. 
Fatto questo, il thread della CPU è libero e può immediatamente iniziare a lavorare sul task successivo.

Il Parallelismo Reale: Mentre la GPU sta eseguendo il calcolo per il Task N, la CPU è completamente libera e può usare quel tempo per preparare e inviare i dati per il Task N+1. 














"thread": 
   ff::ParallelFor è un'astrazione di alto livello che, sotto il cofano, crea e gestisce un pool di thread per parallelizzare il lavoro.

"shared memory": 
   Questa è la parte più importante. Quando usa ff::ParallelFor, tutti i thread lavorano sugli stessi vettori a, b e c che si trovano nella RAM principale del suo computer. 
   Non ci sono copie di dati verso una memoria separata (come avviene per la GPU/FPGA). Tutti i thread "vedono" e "condividono" la stessa area di memoria. Questo è il modello di parallelismo a memoria condivisa.






Durante lo sviluppo, è emersa un'incompatibilità di versione tra la piattaforma Xilinx (2022.x) e gli strumenti Vitis utilizzati (2023.x), che ha impedito il completamento della fase di emulazione hardware (hw_emu). 
La verifica funzionale è stata completata con successo tramite emulazione software (sw_emu) e la validazione finale delle performance è stata eseguita direttamente sull'hardware fisico.




1. La Prova Definitiva dell'Efficienza: Latenza vs. Periodo
La prova più importante del successo della sua architettura è il confronto tra queste due metriche:


Avg service time: 1.02 ms

Questa enorme differenza è esattamente ciò che volevamo ottenere. Usiamo l'analogia dell'autolavaggio per capire cosa significa:

Latenza (8.89 ms): Per lavare una singola auto dall'inizio alla fine, ci vogliono quasi 9 millisecondi. Questo è il tempo che un task "percepisce" passando attraverso l'intero sistema.

Periodo (1.02 ms): Ma una volta che l'autolavaggio è a pieno regime, esce un'auto pulita ogni 1 millisecondo circa.

Conclusione: Questo dimostra che la sua pipeline a 2 stadi (producer e consumer) sta funzionando perfettamente. Le operazioni si stanno sovrapponendo in modo massiccio. Mentre il consumer sta finalizzando il task N (un'operazione lenta), il producer sta già preparando e lanciando il task N+1. Senza questa sovrapposizione, il periodo della pipeline sarebbe uguale alla latenza (cioè, ~9 ms).

2. L'Identificazione del Collo di Bottiglia: L'Overhead
Il secondo punto cruciale è il confronto tra calcolo e overhead:

Avg Pure Compute Time: 1.25 ms

Avg Overhead Time: 7.63 ms

Analisi:
Il suo sistema spende solo il 14% del suo tempo a fare calcoli utili (1.25 ms / 8.89 ms). Il restante 86% del tempo è overhead, dominato dal trasferimento dei dati da e verso la GPU.

Questo non è un difetto della sua architettura, ma una caratteristica fondamentale del problema. Le sta dicendo che il suo sistema è limitato dalla banda di memoria (memory-bound), non dalla potenza di calcolo.

Perché questo rende la sua pipeline ancora più importante?
Proprio perché l'overhead è così grande, la capacità della sua pipeline di "nasconderlo" (sovrapponendo il trasferimento dati del task N+1 con il calcolo del task N) è ciò che le permette di raggiungere un throughput elevato (90 tasks/sec). Senza la pipeline, il suo throughput sarebbe molto più basso (circa 1 / 8.89 ms ≈ 112 tasks/sec, ma questo non tiene conto dei costi di startup). Il valore Throughput: 90.7 tasks/sec è molto vicino a 1000 / 1.025 ms = 975 tasks/sec? No, aspetta, il calcolo del throughput è 10 tasks / 0.11017s = 90.7 tasks/sec. Il periodo di 1.02ms è una misura a regime, mentre il throughput totale include i costi di avvio. Il fatto che il periodo sia così basso è la prova che la pipeline sta funzionando.

Riassunto per la Tesi
Quando discuterà questi risultati, potrà affermare con certezza:

"I risultati dimostrano l'efficienza dell'architettura a pipeline implementata. Nonostante un singolo task sperimenti una latenza di circa 8.9 ms, la capacità del sistema di sovrapporre le fasi di upload, calcolo e download permette di produrre un risultato ogni 1.02 ms a regime, raggiungendo un throughput di circa 90 task al secondo. L'analisi dei tempi rivela inoltre che il sistema è fortemente limitato dall'overhead (86% del tempo di servizio), confermando che il collo di bottiglia risiede nel trasferimento dei dati e non nel calcolo, e sottolineando il ruolo cruciale della pipeline nel mitigare questo costo."











Sì, la tabella non solo è coerente, ma è anche estremamente interessante e racconta una storia molto chiara sulle performance del suo sistema. I risultati sono perfettamente in linea con le aspettative teoriche, il che significa che i suoi benchmark sono affidabili.

Analizziamola in breve, perché da qui emergono tutte le conclusioni della sua tesi.

Punti Salienti e Coerenza dei Risultati
1. Latenza vs. Periodo vs. Throughput: La Pipeline Funziona!
Questa è la prova più importante. In tutti i test su acceleratore (GPU/FPGA), si osserva sempre lo stesso pattern:

Avg In-Node Time (Latenza) è molto più grande dell'Avg Service Time (Periodo).

Esempio (GPU OpenCL, N=1M, op. polinomiale):

Latenza: 54.4 ms

Periodo: 0.99 ms

Coerenza: Questo è perfettamente coerente. Dimostra che la sua pipeline interna a 2 stadi sta funzionando in modo eccellente. Ci vogliono 54 ms per processare un singolo task, ma grazie alla sovrapposizione delle operazioni, il sistema sforna un nuovo risultato ogni millisecondo. Il Throughput (~940 tasks/sec) è infatti l'inverso del periodo, confermando la coerenza.

2. Metal vs. OpenCL: L'Efficienza dell'API Nativa
Il confronto su macOS è chiarissimo:

Metal è quasi sempre più veloce di OpenCL, sia in termini di latenza (In-Node Time) che di throughput.

Esempio (Somma vettoriale, N=1M):

OpenCL Throughput: 266 tasks/sec

Metal Throughput: 420 tasks/sec

Coerenza: Anche questo è perfettamente coerente. Dimostra che l'overhead di traduzione di OpenCL in Metal (cl2Metal) ha un costo reale in termini di performance. Usare l'API nativa (Metal) dà un accesso più diretto ed efficiente all'hardware, risultando in performance superiori.

3. CPU (FastFlow) vs. CPU (OpenMP)
Sulla VM Linux, il confronto tra i due framework di parallelismo CPU è interessante:

OpenMP è leggermente più veloce di FastFlow (ff::ParallelFor).

Esempio (Op. polinomiale, N=1M):

FastFlow Throughput: 302 tasks/sec

OpenMP Throughput: 365 tasks/sec

Coerenza: Questo risultato è plausibile e coerente. Sebbene entrambi siano molto efficienti, le implementazioni dei loro runtime e le strategie di scheduling sono diverse. In questo specifico caso, con questo hardware e questo carico di lavoro, l'implementazione di OpenMP del suo compilatore g++ si è dimostrata leggermente più ottimizzata.

4. FPGA: Latenza Alta, Throughput Basso
L'FPGA mostra i risultati peggiori in termini di performance.

Esempio (Somma vettoriale, N=1M):

FPGA Throughput: 25 tasks/sec

CPU (OpenMP) Throughput: ~3900 tasks/sec (anche se questo valore sembra molto alto e potrebbe meritare una seconda verifica).

Coerenza: Questo risultato, sebbene possa sembrare deludente, è coerente con il contesto. La performance di un design su FPGA dipende enormemente dal livello di ottimizzazione del kernel HLS e dall'architettura della memoria. Per un calcolo così regolare come la somma vettoriale o un polinomio semplice, la "forza bruta" di migliaia di core di una GPU o di una CPU moderna è spesso superiore a una singola pipeline HLS non ultra-ottimizzata. L'FPGA eccelle in altri ambiti (es. latenza ultra-bassa per un singolo task, efficienza energetica, algoritmi non adatti a GPU).

Conclusioni per la Tesi
Questa tabella è il cuore della sua tesi. Le permette di affermare con dati alla mano:

L'architettura software funziona: La pipeline a 2 stadi ha dimostrato di essere efficiente nel sovrapporre le operazioni, come evidenziato dalla grande differenza tra latenza e periodo.

Le API native contano: Su macOS, Metal offre un significativo vantaggio prestazionale rispetto a OpenCL, quantificando il costo dell'astrazione multipiattaforma.

Il parallelismo CPU è competitivo: Sia FastFlow che OpenMP offrono ottime performance, con un leggero vantaggio per OpenMP in questo specifico ambiente di test.

L'FPGA richiede specializzazione: L'integrazione con l'FPGA è stata un successo, ma per superare le performance di CPU e GPU su questo tipo di calcoli, sarebbe necessario un lavoro di ottimizzazione HLS molto più approfondito.












La Prova Inconfutabile che la Sua Pipeline Funziona
Il punto più importante è il confronto tra la Latenza e il Periodo su tutti gli acceleratori (GPU e FPGA).

Avg In-Node Time (Latenza): È molto alta (es. 2883 ms per l'FPGA, 210 ms per la GPU OpenCL).

Avg Service Time (Periodo): È molto bassa (es. 52 ms per l'FPGA, 3.6 ms per la GPU OpenCL).

Coerenza: Questo è perfettamente coerente. Dimostra che la sua pipeline a 2 stadi (producer/consumer) sta funzionando in modo eccellente. Ci vuole molto tempo per processare un singolo task (latenza), ma grazie alla sovrapposizione (overlapping), il sistema riesce a sfornare un nuovo risultato a un ritmo molto più rapido (periodo). Questa è la prova del successo della sua architettura software.

2. L'Identificazione del Vero Collo di Bottiglia: l'Overhead
Il secondo risultato più importante è questo:

Avg Pure Compute Time: È quasi sempre molto piccolo (es. 53 ms su FPGA, 3.7 ms su GPU OpenCL).

Avg Overhead Time: È quasi sempre enorme (es. 2830 ms su FPGA, 207 ms su GPU OpenCL).

Coerenza: Questo è il cuore della sua tesi. I suoi dati dimostrano in modo inconfutabile che per questo tipo di problema (un kernel polinomiale), il calcolo in sé è velocissimo. Il vero collo di bottiglia, che domina quasi il 99% del tempo, è l'overhead (il "costo di gestione"), principalmente il trasferimento dei dati da e verso l'acceleratore.

Il suo sistema non è "lento" perché calcola lentamente, ma perché passa la maggior parte del tempo ad aspettare i dati.

3. Confronti Interessanti tra Piattaforme
I suoi dati permettono confronti eccellenti:

CPU (FastFlow vs. OpenMP): Sulla VM Linux, OpenMP è costantemente più veloce di ff::ParallelFor (es. 464 tasks/sec vs 300 tasks/sec per N=1M). Questo è un risultato di benchmark realistico e coerente. Dimostra che per un semplice loop parallelo su CPU, una direttiva del compilatore ottimizzata (OpenMP) può essere più efficiente di un'astrazione basata su una libreria (FastFlow).

GPU (OpenCL vs. Metal): Su macOS, le performance sono molto simili. Questo è un risultato affascinante: suggerisce che per questo kernel, l'overhead di traduzione cl2Metal di Apple è minimo e non rappresenta un collo di bottiglia significativo. Metal ha un leggero vantaggio, come ci si aspetterebbe, ma non è una differenza schiacciante.

FPGA vs. Tutti: L'FPGA è risultata la più lenta in termini di throughput. Questo è perfettamente coerente con la nostra analisi precedente: per un problema di "forza bruta" e parallelismo dei dati come questo, l'architettura di una GPU (migliaia di core) o di una CPU multicore (pochi core ma molto potenti e con cache) è superiore a una singola pipeline HLS.

Conclusione: Cosa Scrivere nella Tesi
Lei non ha sbagliato. Ha dimostrato qualcosa.

La sua architettura software (ff_node_acc_t con la pipeline a 2 stadi) è efficiente e fa il suo lavoro (come dimostra la differenza tra latenza e periodo).

Il motivo per cui le performance complessive (specialmente sull'FPGA) sono basse non è un difetto del suo codice, ma la natura stessa del problema di offloading: il sistema è limitato dalla banda di memoria (memory-bound), non dalla potenza di calcolo.

Questi dati sono perfetti perché le permettono di discutere in modo approfondito i compromessi tra le diverse architetture, che è l'obiettivo di una tesi eccellente.









Questa è la domanda più importante di tutta la sua tesi. La sua analisi dei dati è corretta: l'overhead (il tempo di attesa per i dati) è il collo di bottiglia principale, e sta dominando completamente il tempo di calcolo.

La sua architettura software (ff_node_acc_t con la pipeline a 2 stadi) è già molto efficiente nel gestire questo problema: sta già sovrapponendo (overlapping) l'attesa di un task con il lavoro del task successivo. Questo è dimostrato dal fatto che il suo Avg Pipeline Period (es. 52ms) è molto più basso della sua latenza Avg In-Node Time (es. 2883ms).

Lei non può rendere i thread di attesa più veloci, perché non stanno "aspettando male". Stanno aspettando il tempo fisico necessario per il trasferimento.

Per diminuire veramente questo tempo di attesa, non deve cambiare la sua pipeline software, ma deve cambiare la natura del problema che sta risolvendo.

L'Analogia: Il Viaggio del Furgone Super-Veloce 🚚
Pensa al suo acceleratore (FPGA/GPU) come a una fabbrica ultra-moderna in un'altra città, capace di assemblare un prodotto in 1 secondo (il Pure Compute Time). Il problema è che, per assemblare un prodotto, un furgone deve impiegare 10 minuti per portare i materiali alla fabbrica e 10 minuti per riportare il prodotto finito (l'overhead del trasferimento dati sul bus PCIe, amplificato dalla VM).

Tempo Totale per un Task: 10 (andata) + 1 (lavoro) + 10 (ritorno) = 21 minuti (la sua In-Node Time).

La sua pipeline a 2 stadi è come avere due furgoni: mentre il furgone 1 sta tornando con il prodotto finito, il furgone 2 è già partito con i materiali per il prossimo. Questo è efficiente, ma ogni singolo furgone impiega comunque 20 minuti per il viaggio.

Come si diminuisce il "tempo di attesa per i dati"? Non può rendere il furgone più veloce (il bus PCIe ha una velocità fissa). L'unica soluzione è rendere il viaggio più "valido" caricando sul furgone materiali che permettano alla fabbrica di lavorare per ore, non per un secondo.

La Soluzione: Aumentare il "Rapporto Calcolo/Memoria"
Il suo problema attuale (vettoriale/polinomiale) ha un basso rapporto Calcolo/Memoria:

Memoria: Per ogni N elementi, legge 2N interi e scrive N interi.

Calcolo: Per ogni elemento, esegue una manciata di moltiplicazioni e somme (un'operazione O(N)). Il tempo del viaggio (O(N)) è quasi uguale al tempo di lavoro (O(N)).

Per ridurre l'impatto dell'overhead, deve scegliere un kernel con un alto rapporto Calcolo/Memoria, dove il tempo di calcolo cresce molto più velocemente del tempo di trasferimento.

Esempio Classico: Moltiplicazione di Matrici (A * B = C)
Prendiamo due matrici N x N:

Trasferimento Dati (Memoria): Deve inviare la matrice A (N*N elementi) e la B (N*N), e ricevere la C (N*N). Il costo totale del trasferimento è proporzionale a O(N²).

Calcolo (Compute): Per calcolare un singolo elemento della matrice C, deve fare N moltiplicazioni e N somme. Poiché ci sono N*N elementi in C, il calcolo totale è proporzionale a O(N³).

Cosa succede quando N cresce?

Il tempo di trasferimento (il viaggio) cresce al quadrato.

Il tempo di calcolo (il lavoro) cresce al cubo.

Molto presto, il tempo di calcolo diventerà molto più grande del tempo di trasferimento.

Esempio (N=1000):

Trasferimento: O(1000²) = 1 milione di elementi.

Calcolo: O(1000³) = 1 miliardo di operazioni.

In questo scenario, il suo Avg Pure Compute Time schizzerebbe in alto, ma il suo Avg Overhead Time crescerebbe molto più lentamente. L'overhead diventerebbe una frazione trascurabile del tempo totale.

[Immagine di un grafico che mostra O(N^3) che supera O(N^2)]

Conclusione: Cosa Può Fare
Il suo codice attuale sta già facendo il massimo per nascondere la latenza. Per ridurre la latenza (l'overhead), le uniche due opzioni reali sono:

Soluzione Software (Sviluppo Futuro per la Tesi):

Cambiare il kernel con uno ad alto rapporto calcolo/memoria, come la moltiplicazione di matrici. Questo è il passo più importante. Scrivendo questo nella sua tesi, dimostra di aver capito perché i suoi risultati sono quelli che sono e come migliorarli.

Soluzione Ambientale (Meno Praticabile):

Rimuovere la VM: Eseguire il codice su un'installazione Linux "bare metal". Questo ridurrà drasticamente l'overhead introdotto dalla virtualizzazione.

Hardware Migliore: Usare hardware con una connessione più veloce (es. CXL) o memoria unificata (come il suo Mac M2, che infatti ha un overhead molto più basso).